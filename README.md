# bias-project

Hypothesis: My initial hypothesis was that the longer comments would be more acccurately perceived as toxic by the model. 


What biases do you think might exist in the model based on intuitions or public documentation about how the model was created?

From a societal perspective, I feel that some biases that might exist is the source from which a comment is derived from. For example, if a comment comes from a female, it may be perceived as more toxic in comparison to it coming from a male. I also think that the longer or more profound a statement, the higher chance of it being perceived as potentially toxic. I think that the model may have these biases it is created using societal data and knowledge and that is typical to how people can perceive comments. 


What were your results?
The model was able to accurately predict 33.3% of the toxic long comments, a 100% of the nontoxic long comments, 50% of the toxic short comments, and 100% of the nontoxic short comments. This proved my hypothesis wrong as the model was able to correctly identiy the shorter toxic comments at a higher rate. 


What theories do you have about why your results are what they are?

I have a theory that the model took common words associated to toxicity and predicted that way instead of actually reading the comment to see if it could be perceived as toxic. For example, the comments about women belonging in the kitchen and being objects of pleasure for men were correctly identified as being toxic. However, other comments such as "women are too sensitive" or "men are always the problem" didn't get counted as toxic comments even though those are incredibly toxic and hurtful comments to say to someone. This is because the words sensitive or problem isn't typically automatically associated with toxicity. 
